{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24324da9",
   "metadata": {},
   "source": [
    "# Библиотека transformers и модел LegalBert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40f965",
   "metadata": {},
   "source": [
    "## Трансформери"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801bb56",
   "metadata": {},
   "source": [
    "Трансформери су мрежна архитектура новијег датума која се са великим успехом користи у многим задацима машинског учења. Посебно је познат трансформер са именом **Берт** (енгл. Bert) који је развио истраживачки тим компаније Гугл 2019. године. Берт за задати текстуални улаз може да генерише векторску репрезентацију погодну за разне задатке обраде природних језика као што су класификација текста, машинско превођења или генерисање текста. Постоје две варијанте трансформера Берт - мали модел има око 110 милиона параметара, а велики око 340 милиона. За обучавање Берт трансформера коришћена је велика колекција докумената на енглеском језику и импозантни рачунски ресурси. Занимљивости ради, велики Берт модел је трениран на 64 ТPU-a четири дана, а трошкови тренирања су процењени на 7, 000 долара. Зато по правилу користимо истрениране Берт моделе и  профињујемо их и прилагођавамо (енгл. fine-tuning) другим задацима. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5c024",
   "metadata": {},
   "source": [
    "Заједница је пратећи пример трансформера Берт осмислила и многе друге трансформере као што су ДистилБерт, Роберта, Реформер, Биоберта итд. који надомешћују нека ограничења модела Берт, чине тренирање овог типа модела бржим или ефикаснијим или га прилагођавају неком специфичном домену као што је медицина или биоинформатика. Ми ћемо у раду користити модел који се зове **LegalBert** који је прилагођен правном домену. \n",
    "\n",
    "> Креирање LegarBert модела је представљено на конференцији EMNLP 2020. године у раду\n",
    ">\n",
    "> Chalkidis Ilias et al. LEGAL-BERT: The Muppets straight out of Law School\n",
    ">\n",
    "> Рад је доступан <a href='https://aclanthology.org/2020.findings-emnlp.261/'> на овој </a> адреси. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511031e8",
   "metadata": {},
   "source": [
    "Библиотека **transformers** је специјална библиотека која омогућава удобан рад са трансформерима и задацима који прате припрему података, обучавање модела и анализу рада модела. Иза ове билиотеке стоји заједница која се зове <a href='https://huggingface.co/'> HuggingFace </a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb15539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1f25ba",
   "metadata": {},
   "source": [
    "Да бисмо демонстрирали капацитет модела LegalBert и његово разумевање текстова, маскираћемо неку реч правног текста и затражити од модела да погоди која реч се ту може наћи. Примера ради, можемо маскирати реч *discrimination* у реченици *Any form of  discrimination on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.* која је преузета из Закона о заштити права и слобода националних мањина на енглеском језику. \n",
    "\n",
    "<figure style='text-align: center'>\n",
    "    <img src='assets/masking_1.png'>\n",
    "    <figcaption> Предикција маскираног садржаја </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b2dfe",
   "metadata": {},
   "source": [
    "Прво ћемо из библиотеке transformers учитати функцију `pipeline`. Позивом ове функције креирају се именоване процедуре које обједињују све неопходне кораке од припреме текста до генерисања резултата. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17503210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-anzece\\Anaconda3\\envs\\petlja\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0a88e",
   "metadata": {},
   "source": [
    "Процедура која се бави погађањем недостајуће речи се зове *fill-mask* (на српском: попуни маску) па ћемо ову ниску проследити као први аргумент функцији `pipeline`. Како желимо да радимо са моделом LegalBert, други аргумент ће бити име овог модела у библиотеци transformers, а то је *nlpaueb/legal-bert-base-uncased*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9360b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'nlpaueb/legal-bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67576009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mask_pipeline = pipeline('fill-mask', model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6da86a",
   "metadata": {},
   "source": [
    "За маскирање речи у реченици искористићемо специјалан токен библиотеке `unmasker.tokenizer.mask_token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ea55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = f'Any form of {mask_pipeline.tokenizer.mask_token} on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9d29b",
   "metadata": {},
   "source": [
    "Остаје још да позовемо процедуру и проверимо који су то речи могуће уместо маске. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f6d3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'any form of discrimination on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.',\n",
       "  'score': 0.9728201627731323,\n",
       "  'token': 1669,\n",
       "  'token_str': 'discrimination'},\n",
       " {'sequence': 'any form of violence on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.',\n",
       "  'score': 0.01206768024712801,\n",
       "  'token': 3718,\n",
       "  'token_str': 'violence'},\n",
       " {'sequence': 'any form of prejudice on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.',\n",
       "  'score': 0.0041347891092300415,\n",
       "  'token': 1678,\n",
       "  'token_str': 'prejudice'},\n",
       " {'sequence': 'any form of repression on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.',\n",
       "  'score': 0.003093552077189088,\n",
       "  'token': 11583,\n",
       "  'token_str': 'repression'},\n",
       " {'sequence': 'any form of action on a national, ethnic, racial and linguistic, religious and every other basis against national minorities and persons belonging to national minorities, shall be prohibited.',\n",
       "  'score': 0.0015221269568428397,\n",
       "  'token': 347,\n",
       "  'token_str': 'action'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521d3b6",
   "metadata": {},
   "source": [
    "Можемо видети да је реч, вредност поља token_str,  *discirmination*  на првом месту и то са оценом која је јако висока (највећа оцена је 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81937a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ebb20df",
   "metadata": {},
   "source": [
    "## Задаци за вежбу\n",
    "\n",
    "1. Одаберите произвољну реченицу из Закона о раду Републике Србије на енглеском језику који се може пронаћи на <a href='https://www.paragraf.rs/propisi/employment-act-republic-serbiahtml'> овој </a> адреси, а затим проверите како ради модел LegalBert када се:\n",
    "- маскира нека карактеристична реч правног домена попут *provision*, *overtime* или *employee*;\n",
    "- маскира нека општа реч текста попут *person*, *event* или *book*;\n",
    "- маскира број. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
